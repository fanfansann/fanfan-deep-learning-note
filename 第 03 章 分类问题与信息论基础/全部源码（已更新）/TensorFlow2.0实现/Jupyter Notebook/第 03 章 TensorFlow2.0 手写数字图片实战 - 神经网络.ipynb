{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67527f3f",
   "metadata": {},
   "source": [
    "$❑\\, \\, $ **Step 0 引入文件并设置图像参数**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9d66ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# “在 jupyter notebook 在线使用 matplotlib”，为 IPython 的内置 magic 函数，在 Pycharm 中不被支持\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.datasets as datasets\n",
    "\n",
    "plt.rcParams['font.size'] = 16\n",
    "plt.rcParams['font.family'] = ['STKaiti']\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cd7f0c",
   "metadata": {},
   "source": [
    "$❑\\, \\, $ **Step 1 导入数据并对数据进行处理**\n",
    "\n",
    "&emsp;&emsp;利用 TensorFlow 自动在线下载 MNIST 数据集，并转换为 Numpy 数组格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afd312bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data() :\n",
    "    # 加载 MNIST 数据集 元组tuple: (x, y), (x_val, y_val)\n",
    "    (x, y), (x_val, y_val) = datasets.mnist.load_data()\n",
    "    # 将 x 转换为浮点张量，并从 0 ~ 255 缩放到 [0, 1.] - 1 -> [-1, 1] 即缩放到 -1 ~ 1\n",
    "    x = tf.convert_to_tensor(x, dtype = tf.float32) / 255. - 1\n",
    "    # 转换为整形张量\n",
    "    y = tf.convert_to_tensor(y, dtype = tf.int32)\n",
    "    # one-hot 编码\n",
    "    y = tf.one_hot(y, depth = 10)\n",
    "    # 改变视图， [b, 28, 28] => [b, 28*28]\n",
    "    x = tf.reshape(x, (-1, 28 * 28))\n",
    "    # 构建数据集对象\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    # 批量训练\n",
    "    train_dataset = train_dataset.batch(200)\n",
    "    return train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22707823",
   "metadata": {},
   "source": [
    "\n",
    "&emsp;&emsp; TensorFlow 中的 `load_data()` 函数返回两个**元组** (tuple) 对象，第一个是训练集，第二个是测试集，每个 tuple 的第一个元素是多个训练图片数据 $X$ ，第二个元素是训练图片对应的类别数字 $Y$。其中训练集 $X$ 的大小为 $(60000,28,28)$ ，代表了 $60000$ 个样本，每个样本由 $28$ 行、$28$ 列构成，由于是灰度图片，故没有 RGB 通道；训练集 $Y$ 的大小为 $(60000)$，代表了这 $60000$ 个样本的标签数字，每个样本标签用一个  $0\\sim 9$  的数字表示，测试集同理。\n",
    "\n",
    "&emsp;&emsp;从 TensorFlow 中加载的 MNIST 数据图片，数值的范围在 $[0,255]$ 之间。在机器学习中间，一般希望数据的范围在 $0$ 周围小范围内分布。我们可以通过预处理步骤，我们把  $[0,255]$ 像素范围**归一化**(Normalize)到 $[0,1.]$ 区间，再缩放到 $[−1,1]$ 区间，从而有利于模型的训练。\n",
    "\n",
    "&emsp;&emsp;每一张图片的计算流程是通用的，我们在计算的过程中可以一次进行多张图片的计算，充分利用 CPU 或 GPU 的并行计算能力。一张图片我们用 shape 为 $[h, w]$ 的矩阵来表示，对于多张图片来说，我们在前面添加一个**数量维度** （Dimension），使用 shape 为 $[b, h, w]$ 的张量来表示，其中的 $b$ 代表了 batch size(**批量**。多张彩色图片可以使用 shape 为 $[b, h, w, c]$ 的张量来表示，其中的 $c$ 表示通道数量（Channel），彩色图片$c = 3$（R、G、B）。通过 TensorFlow 的Dataset 对象可以方便完成模型的批量训练，只需要调用 `batch()` 函数即可构建带 `batch` 功能的数据集对象。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4478bfdb",
   "metadata": {},
   "source": [
    "$❑\\, \\, $ **Step 2 网络搭建**\n",
    "\n",
    "\n",
    "&emsp;&emsp;对于第一层模型来说，他接受的输入 $𝒙 ∈ \\mathbb R^{784}$ ，输出𝒉𝟏 ∈ $\\mathbb R^{256}$ 设计为长度为 $256$ 的向量，我们不需要显式地编写 $\\boldsymbol{h}_{1}=\\operatorname{ReLU}\\left(\\boldsymbol{W}_{1} \\boldsymbol{x}+\\boldsymbol{b}_{1}\\right)$ 的计算逻辑，在 TensorFlow 中通过一行代码即可实现：\n",
    "\n",
    "\n",
    "```python\n",
    "layers.Dense(256, activation = 'relu') \n",
    "```\n",
    "\n",
    "&emsp;&emsp;使用 TensorFlow 的 Sequential 容器可以非常方便地搭建多层的网络。对于 3 层网络，我们可以通过\n",
    "\n",
    "```python\n",
    "keras.sequential([\n",
    "    layers.Dense(256, activation = 'relu'),\n",
    "    layers.Dense(128, activation = 'relu'),\n",
    "    layers.Dense(10)])\n",
    "```\n",
    "\n",
    "\n",
    "&emsp;&emsp;快速完成 $3$ 层网络的搭建，第 $1$ 层的输出节点数设计为 $256$，第 $2$ 层设计为 $128$，输出层节点数设计为 $10$。直接调用这个模型对象 `model(x)` 就可以返回模型最后一层的输出 。\n",
    "\n",
    "&emsp;&emsp;为了能让大家理解更多的细节，我们这里不使用上面的框架，手动实现经过 3 层神经网络。\n",
    "\n",
    "对神经网络参数初始化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96f0eb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_paramaters() :\n",
    "    # 每层的张量需要被优化，使用 Variable 类型，并使用截断的正太分布初始化权值张量\n",
    "    # 偏置向量初始化为 0 即可\n",
    "    # 第一层参数\n",
    "    w1 = tf.Variable(tf.random.truncated_normal([784, 256], stddev = 0.1))\n",
    "    b1 = tf.Variable(tf.zeros([256]))\n",
    "    # 第二层参数\n",
    "    w2 = tf.Variable(tf.random.truncated_normal([256, 128], stddev = 0.1))\n",
    "    b2 = tf.Variable(tf.zeros([128]))\n",
    "    # 第三层参数\n",
    "    w3 = tf.Variable(tf.random.truncated_normal([128, 10], stddev = 0.1))\n",
    "    b3 = tf.Variable(tf.zeros([10]))\n",
    "    return w1, b1, w2, b2, w3, b3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a242c4",
   "metadata": {},
   "source": [
    "$❑\\, \\, $ **Step 3 模型训练**\n",
    "\n",
    "&emsp;&emsp;得到模型输出 $\\boldsymbol{o}$ 后，通过 MSE 损失函数计算当前的误差 $\\mathcal L$：\n",
    "\n",
    "```python\n",
    "with tf.GradientTape() as tape:#构建梯度记录环境\n",
    "    #打平，[b,28,28] =>[b,784]\n",
    "    x=tf.reshape(x,(-1,28*28))\n",
    "    #step1. 得到模型输出 output\n",
    "    # [b,784] =>[b,10]\n",
    "    out=model(x)\n",
    "```\n",
    "\n",
    "**手动实现代码：**\n",
    "\n",
    "\n",
    "```python\n",
    "    with tf.GradientTape() as tape :#构建梯度记录环境\n",
    "            # 第一层计算， [b, 784]@[784, 256] + [256] => [b, 256] + [256] => [b,256] + [b, 256]\n",
    "            h1 = x @ w1 + tf.broadcast_to(b1, (x.shape[0], 256))\n",
    "            # 通过激活函数 relu\n",
    "            h1 = tf.nn.relu(h1)\n",
    "            # 第二层计算， [b, 256] => [b, 128]\n",
    "            h2 = h1 @ w2 + b2\n",
    "            h2 = tf.nn.relu(h2)\n",
    "            # 输出层计算， [b, 128] => [b, 10]\n",
    "            out = h2 @ w3 + b3\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00432bee",
   "metadata": {},
   "source": [
    "$❑\\, \\, $ **Step 4 梯度优化**\n",
    "\n",
    "&emsp;&emsp;利用 TensorFlow 提供的自动求导函数 `tape.gradient(loss, model.trainable_variables)` 求出模型中所有的梯度信息 $\\dfrac{\\partial L}{\\partial \\theta}$  , $\\theta ∈ \\{\\boldsymbol W_1, 𝒃_𝟏,\\boldsymbol W_2, 𝒃_𝟐,\\boldsymbol W_3, 𝒃_𝟑\\}$：\n",
    "\n",
    "```python \n",
    " grads = tape.gradient(loss, model.trainable_variables) \n",
    "```\n",
    "\n",
    "&emsp;&emsp;计算获得的梯度结果使用 grads 变量保存。再使用 optimizers 对象自动按着梯度更新法则\n",
    "$$\n",
    "\\theta^{\\prime}=\\theta-\\eta \\times  \\frac{\\partial \\mathcal{L}}{\\partial \\theta}\n",
    "$$\n",
    "\n",
    "\n",
    "去更新模型的参数 $\\theta$。\n",
    "\n",
    "```python\n",
    "grads = tape.gradient(loss, model.trainable_variables)\n",
    " # w' = w - lr * grad，更新网络参数\n",
    "optimizer.apply_gradients(zip(grads, model.trainable_variables)) \n",
    "```\n",
    "\n",
    "&emsp;&emsp;循环迭代多次后，就可以利用学好的模型 $𝑓_{\\theta}$ 去预测未知的图片的类别概率分布。 \n",
    "\n",
    "\n",
    "**手动实现梯度更新代码如下：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36967ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch, train_dataset, w1, b1, w2, b2, w3, b3, lr = 0.001) :\n",
    "    for step, (x, y) in enumerate(train_dataset) :\n",
    "        with tf.GradientTape() as tape :\n",
    "            # 第一层计算， [b, 784]@[784, 256] + [256] => [b, 256] + [256] => [b,256] + [b, 256]\n",
    "            h1 = x @ w1 + tf.broadcast_to(b1, (x.shape[0], 256))\n",
    "            # 通过激活函数 relu\n",
    "            h1 = tf.nn.relu(h1)\n",
    "            # 第二层计算， [b, 256] => [b, 128]\n",
    "            h2 = h1 @ w2 + b2\n",
    "            h2 = tf.nn.relu(h2)\n",
    "            # 输出层计算， [b, 128] => [b, 10]\n",
    "            out = h2 @ w3 + b3\n",
    "\n",
    "            # 计算网络输出与标签之间的均方差， mse = mean(sum(y - out) ^ 2)\n",
    "            # [b, 10]\n",
    "            loss = tf.square(y - out)\n",
    "            # 误差标量， mean: scalar\n",
    "            loss = tf.reduce_mean(loss) \n",
    "            # 自动梯度，需要求梯度的张量有[w1, b1, w2, b2, w3, b3]\n",
    "            grads = tape.gradient(loss, [w1, b1, w2, b2, w3, b3])\n",
    "\n",
    "        # 梯度更新， assign_sub 将当前值减去参数值，原地更新\n",
    "        w1.assign_sub(lr * grads[0])\n",
    "        b1.assign_sub(lr * grads[1])\n",
    "        w2.assign_sub(lr * grads[2])\n",
    "        b2.assign_sub(lr * grads[3])\n",
    "        w3.assign_sub(lr * grads[4])\n",
    "        b3.assign_sub(lr * grads[5])\n",
    "\n",
    "        if step % 100 == 0 :\n",
    "            print(f\"epoch:{epoch}, iteration:{step}, loss:{loss.numpy()}\")   \n",
    "            \n",
    "    return loss.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f47773",
   "metadata": {},
   "source": [
    "整体的代码思路如下：\n",
    "\n",
    "&emsp;&emsp; 我们首先创建每个非线性层的 $𝑾$ 和 $𝒃$ 张量参数，然后把 $\\boldsymbol x$ 的 `shape=[b, 28, 28]` 转成向量 `[b, 784]` ，然后计算三层神经网络，每层使用 ReLU 激活函数，然后与 `one_hot` 编码的 $\\boldsymbol y$ 一起计算均方差，利用 `tape.gradient()` 函数自动求梯度\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\theta^{\\prime}=\\theta-\\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial \\theta}\n",
    "$$\n",
    "\n",
    "\n",
    "&emsp;&emsp;使用 `assign_sub()` 函数按照上述梯度下降算法更新网络参数（assign_sub()将自身减去给定的参数值，实现参数的原地 (In-place) 更新操作），最后使用 `matplotlib` 绘制图像输出即可。 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6703b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, iteration:0, loss:2.8238346576690674\n",
      "epoch:0, iteration:100, loss:0.12282778322696686\n",
      "epoch:0, iteration:200, loss:0.09311732649803162\n",
      "epoch:1, iteration:0, loss:0.08501865714788437\n",
      "epoch:1, iteration:100, loss:0.08542951196432114\n",
      "epoch:1, iteration:200, loss:0.0726892277598381\n",
      "epoch:2, iteration:0, loss:0.0702851265668869\n",
      "epoch:2, iteration:100, loss:0.07322590798139572\n",
      "epoch:2, iteration:200, loss:0.06422527879476547\n",
      "epoch:3, iteration:0, loss:0.06337807327508926\n",
      "epoch:3, iteration:100, loss:0.06658075749874115\n",
      "epoch:3, iteration:200, loss:0.05931859463453293\n",
      "epoch:4, iteration:0, loss:0.05899645760655403\n",
      "epoch:4, iteration:100, loss:0.062211062759160995\n",
      "epoch:4, iteration:200, loss:0.05583002790808678\n",
      "epoch:5, iteration:0, loss:0.05573083460330963\n",
      "epoch:5, iteration:100, loss:0.058985237032175064\n",
      "epoch:5, iteration:200, loss:0.053309373557567596\n",
      "epoch:6, iteration:0, loss:0.053154319524765015\n",
      "epoch:6, iteration:100, loss:0.0565737783908844\n",
      "epoch:6, iteration:200, loss:0.051321178674697876\n",
      "epoch:7, iteration:0, loss:0.05108541622757912\n",
      "epoch:7, iteration:100, loss:0.054654691368341446\n",
      "epoch:7, iteration:200, loss:0.04969591274857521\n",
      "epoch:8, iteration:0, loss:0.0493333600461483\n",
      "epoch:8, iteration:100, loss:0.05307555943727493\n",
      "epoch:8, iteration:200, loss:0.04830818995833397\n",
      "epoch:9, iteration:0, loss:0.047856513410806656\n",
      "epoch:9, iteration:100, loss:0.051712967455387115\n",
      "epoch:9, iteration:200, loss:0.047130417078733444\n",
      "epoch:10, iteration:0, loss:0.046589870005846024\n",
      "epoch:10, iteration:100, loss:0.05050584673881531\n",
      "epoch:10, iteration:200, loss:0.046077027916908264\n",
      "epoch:11, iteration:0, loss:0.04547039046883583\n",
      "epoch:11, iteration:100, loss:0.0494525171816349\n",
      "epoch:11, iteration:200, loss:0.04513201490044594\n",
      "epoch:12, iteration:0, loss:0.04447377845644951\n",
      "epoch:12, iteration:100, loss:0.04850050434470177\n",
      "epoch:12, iteration:200, loss:0.04429565370082855\n",
      "epoch:13, iteration:0, loss:0.043561432510614395\n",
      "epoch:13, iteration:100, loss:0.04763760045170784\n",
      "epoch:13, iteration:200, loss:0.04352030158042908\n",
      "epoch:14, iteration:0, loss:0.04273266717791557\n",
      "epoch:14, iteration:100, loss:0.04685059189796448\n",
      "epoch:14, iteration:200, loss:0.04281007871031761\n",
      "epoch:15, iteration:0, loss:0.04196896404027939\n",
      "epoch:15, iteration:100, loss:0.04612402245402336\n",
      "epoch:15, iteration:200, loss:0.042143866419792175\n",
      "epoch:16, iteration:0, loss:0.04124370589852333\n",
      "epoch:16, iteration:100, loss:0.045436661690473557\n",
      "epoch:16, iteration:200, loss:0.04152608662843704\n",
      "epoch:17, iteration:0, loss:0.04054683819413185\n",
      "epoch:17, iteration:100, loss:0.04479134455323219\n",
      "epoch:17, iteration:200, loss:0.04094816371798515\n",
      "epoch:18, iteration:0, loss:0.039887987077236176\n",
      "epoch:18, iteration:100, loss:0.044195547699928284\n",
      "epoch:18, iteration:200, loss:0.040402255952358246\n",
      "epoch:19, iteration:0, loss:0.039282456040382385\n",
      "epoch:19, iteration:100, loss:0.04364131763577461\n",
      "epoch:19, iteration:200, loss:0.03987015783786774\n",
      "epoch:20, iteration:0, loss:0.03870239108800888\n",
      "epoch:20, iteration:100, loss:0.04311355948448181\n",
      "epoch:20, iteration:200, loss:0.0393809899687767\n",
      "epoch:21, iteration:0, loss:0.03816535696387291\n",
      "epoch:21, iteration:100, loss:0.04260074719786644\n",
      "epoch:21, iteration:200, loss:0.038916587829589844\n",
      "epoch:22, iteration:0, loss:0.03764738887548447\n",
      "epoch:22, iteration:100, loss:0.0421011820435524\n",
      "epoch:22, iteration:200, loss:0.03846241906285286\n",
      "epoch:23, iteration:0, loss:0.03715534508228302\n",
      "epoch:23, iteration:100, loss:0.04162450134754181\n",
      "epoch:23, iteration:200, loss:0.038025762885808945\n",
      "epoch:24, iteration:0, loss:0.03669610247015953\n",
      "epoch:24, iteration:100, loss:0.04115061089396477\n",
      "epoch:24, iteration:200, loss:0.037617530673742294\n",
      "epoch:25, iteration:0, loss:0.036250464618206024\n",
      "epoch:25, iteration:100, loss:0.04068863391876221\n",
      "epoch:25, iteration:200, loss:0.03720514848828316\n",
      "epoch:26, iteration:0, loss:0.0358189232647419\n",
      "epoch:26, iteration:100, loss:0.04023948684334755\n",
      "epoch:26, iteration:200, loss:0.03680950030684471\n",
      "epoch:27, iteration:0, loss:0.03541094437241554\n",
      "epoch:27, iteration:100, loss:0.039796702563762665\n",
      "epoch:27, iteration:200, loss:0.036432769149541855\n",
      "epoch:28, iteration:0, loss:0.035001132637262344\n",
      "epoch:28, iteration:100, loss:0.0393792949616909\n",
      "epoch:28, iteration:200, loss:0.0360533632338047\n",
      "epoch:29, iteration:0, loss:0.03459516167640686\n",
      "epoch:29, iteration:100, loss:0.038972556591033936\n",
      "epoch:29, iteration:200, loss:0.03568875044584274\n",
      "epoch:30, iteration:0, loss:0.034208156168460846\n",
      "epoch:30, iteration:100, loss:0.03857466205954552\n",
      "epoch:30, iteration:200, loss:0.03533806651830673\n",
      "epoch:31, iteration:0, loss:0.03383728861808777\n",
      "epoch:31, iteration:100, loss:0.03818388655781746\n",
      "epoch:31, iteration:200, loss:0.03499047830700874\n",
      "epoch:32, iteration:0, loss:0.03347790986299515\n",
      "epoch:32, iteration:100, loss:0.03780962899327278\n",
      "epoch:32, iteration:200, loss:0.03465589880943298\n",
      "epoch:33, iteration:0, loss:0.0331435389816761\n",
      "epoch:33, iteration:100, loss:0.03744875639677048\n",
      "epoch:33, iteration:200, loss:0.03432944416999817\n",
      "epoch:34, iteration:0, loss:0.03281198441982269\n",
      "epoch:34, iteration:100, loss:0.03710091859102249\n",
      "epoch:34, iteration:200, loss:0.03401472419500351\n",
      "epoch:35, iteration:0, loss:0.03248513862490654\n",
      "epoch:35, iteration:100, loss:0.03677191212773323\n",
      "epoch:35, iteration:200, loss:0.0337025411427021\n",
      "epoch:36, iteration:0, loss:0.03216780349612236\n",
      "epoch:36, iteration:100, loss:0.03644610941410065\n",
      "epoch:36, iteration:200, loss:0.03340121731162071\n",
      "epoch:37, iteration:0, loss:0.03186160326004028\n",
      "epoch:37, iteration:100, loss:0.03613165020942688\n",
      "epoch:37, iteration:200, loss:0.03310254588723183\n",
      "epoch:38, iteration:0, loss:0.03155995160341263\n",
      "epoch:38, iteration:100, loss:0.03582020476460457\n",
      "epoch:38, iteration:200, loss:0.03279929608106613\n",
      "epoch:39, iteration:0, loss:0.031268708407878876\n",
      "epoch:39, iteration:100, loss:0.03551224246621132\n",
      "epoch:39, iteration:200, loss:0.03250761330127716\n",
      "epoch:40, iteration:0, loss:0.03098423406481743\n",
      "epoch:40, iteration:100, loss:0.035210851579904556\n",
      "epoch:40, iteration:200, loss:0.032223351299762726\n",
      "epoch:41, iteration:0, loss:0.030709076672792435\n",
      "epoch:41, iteration:100, loss:0.03492467850446701\n",
      "epoch:41, iteration:200, loss:0.03194170445203781\n",
      "epoch:42, iteration:0, loss:0.03045233152806759\n",
      "epoch:42, iteration:100, loss:0.03464159741997719\n",
      "epoch:42, iteration:200, loss:0.031668927520513535\n",
      "epoch:43, iteration:0, loss:0.030199026688933372\n",
      "epoch:43, iteration:100, loss:0.03436439484357834\n",
      "epoch:43, iteration:200, loss:0.03139331936836243\n",
      "epoch:44, iteration:0, loss:0.02995467558503151\n",
      "epoch:44, iteration:100, loss:0.03409324958920479\n",
      "epoch:44, iteration:200, loss:0.031120551750063896\n",
      "epoch:45, iteration:0, loss:0.029712168499827385\n",
      "epoch:45, iteration:100, loss:0.03382962942123413\n",
      "epoch:45, iteration:200, loss:0.03084247186779976\n",
      "epoch:46, iteration:0, loss:0.029475683346390724\n",
      "epoch:46, iteration:100, loss:0.03357085585594177\n",
      "epoch:46, iteration:200, loss:0.030582088977098465\n",
      "epoch:47, iteration:0, loss:0.029252037405967712\n",
      "epoch:47, iteration:100, loss:0.033312439918518066\n",
      "epoch:47, iteration:200, loss:0.030324475839734077\n",
      "epoch:48, iteration:0, loss:0.02903318777680397\n",
      "epoch:48, iteration:100, loss:0.033065315335989\n",
      "epoch:48, iteration:200, loss:0.030078813433647156\n",
      "epoch:49, iteration:0, loss:0.028811004012823105\n",
      "epoch:49, iteration:100, loss:0.03282175958156586\n",
      "epoch:49, iteration:200, loss:0.02983343042433262\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def train(epochs) :\n",
    "    losses = []\n",
    "    train_dataset = load_data()\n",
    "    w1, b1, w2, b2, w3, b3 = init_paramaters()\n",
    "    for epoch in range(epochs) :\n",
    "        loss = train_epoch(epoch, train_dataset, w1, b1, w2, b2, w3, b3, lr = 0.01)\n",
    "        losses.append(loss)\n",
    "    x = [i for i in range(0, epochs)]\n",
    "    # 绘制曲线\n",
    "    plt.plot(x, losses, color = 'cornflowerblue', marker = 's', label = '训练', markersize='3')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.legend()\n",
    "    plt.savefig('MNIST数据集的前向传播训练误差曲线.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "if __name__ == '__main__' :\n",
    "\t# x 轴 0 ~ 50\n",
    "    train(epochs = 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
